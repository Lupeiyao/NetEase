Wide&Deep是Google在2016年发布的推荐系统模型，核心思想是利用线性模型的记忆能力和DNN的泛化能力提升性能。

已经应用到Google APP stroe，并且整合到了TensorFlow中。

#### 背景

推荐系统的主要挑战是Memorization和Generalization

- Memorization：记忆相似推荐

  一般利用LR的推荐系统需要对特征进行交叉，这种方法的优点是快、可解释、可分布式。缺点是过拟合，无法捕捉样本中未出现的特征组合，需要人工。

- Generalization：推荐新未出现过的item

  模型一般使用低维度的特征表示来捕获特征之间的相关性，比如FM模型。优点是不用人工参与，缺点是对于特殊的、小众的特征，很难学习到有用

  的低维表示，会导致Over-Generalization，给每个小众内容的排序是相似的，而不是针对每个人进行小众推荐。

#### Wide&Deep解决方案

Wide&Deep分别对应Memorization和Generalization这两部分，结构图如下。

![](C:\Users\Lunus\Desktop\推荐系统\图\Wide&Deep架构图.jpg)

- Wide部分即普通的LR，其中$x$和$\phi(x)$分别表示原始特征和交叉特征，只是交叉项不是所有特征，而是人工选择的部分特征。原文的复杂公式可以表示多阶的特征组合，这里就不写那么复杂了。
  $$
  \begin{align}
  y &= w^T[x,\phi(x)]+b \\
  &= w_0 + \sum_{i=1}^nw_ix_i+\sum_{i=1}^n\sum_{j=i+1}^n\phi(i,j)x_ix_j
  \end{align}
  $$

- Deep部分则是有Embedding的前馈神经网络。其中激活函数用ReLU。$l$指MLP的第$l$层
  $$
  a^{(l+1)}=f(W^{(l)}a^{(l)}+b^{(l)})
  $$

- 最后利用LR再结合一次，Deep共有$l_f$层，$W_{deep}^T$是最有一层的权重。
  $$
  p(y=1|x)=\sigma(W^T[x,\phi (x)]+W_{deep}^Ta^{l_f}+b)
  $$
  

#### 训练

Wide&Deep采用的是联合训练，即在每次mini-batch时同时训练Wide和Deep两个部分，而不同于集成学习分别训练。

Wide部分使用L1正则并用FTRL训练，Deep部分则使用AdaGrad训练。

#### 实验

场景是Google Play商店的app推荐，当用户访问时会推送app（item、impression），所有的app首先要经过一个过滤池保留大约O（10）个app，然后再排序.

本文是针对过滤后的app排序，输入$x$包括$<user,context,impression>​$，返回概率（排序）。

![](C:\Users\Lunus\Desktop\推荐系统\图\Wide&Deep实验结构.jpg)

- 样本5000亿。。。。。。。
- 类别特征，过滤出现m次以下的特征取值
- 连续特征，用CDF调整为[0-1]，先将取值分为n份，第i份的取值规范为$\frac{i-1}{n-1}​$
- 类别特征用32维向量embedding，加上连续特征共1200维
- Wide部分只选取当前app与用户下载的app作交叉，其他的特征不做。
- 模型更新通过热启动，即初始参数选择上次的的训练结果。

#### 总结

- 详细解释了目前常用的 Wide 与 Deep 模型各自的优势：Memorization 与 Generalization。
- 结合 Wide 与 Deep 的优势，提出了联合训练的 Wide & Deep Learning。相比单独的 Wide / Deep模型，实验显示了Wide & Deep的有效性，并成功将之成功应用于Google Play的app推荐业务。
- 目前Wide 结合 Deep的思想已经非常流行，结构虽然简单，从业界的很多反馈来看，合理地结合自身业务借鉴该结构，实际效果确实是efficient。
- Google大佬就是有钱。- -！