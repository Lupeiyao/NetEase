PNN是2016年提出来的，PNN提出了一层特殊的Product layer进行特征交叉，提取高阶特征组合。

#### 背景

FNN在NN层的前面是FM向量的拼接，并且FM需要预训练，模型的效果很大程度上取决于FM的效果，而且向量拼接不利于高阶特征组合的提取。

PNN通过在Embedding层后面增加了一个Product Layer来捕捉高阶特征组合。

#### PNN解决方案

![](C:\Users\Lunus\Desktop\推荐系统\图\PNN架构图.jpg)

- 输出是sigmod函数，两个隐层，神经元数分别是$D_1和D_2$
  $$
  \begin{align}
  \hat{y}&=\sigma(W_3l_2+b_3)\\
  l_2&=ReLU(W_2l_1+b2)\\
  l_1&=ReLU(l_z+l_p+b_1)
  \end{align}
  $$

- 交叉层是关键，生成的输出向量分别是$l_z和l_p，都是D_1维数据​$
  $$
  l_z=(l_z^1,l_z^2,...,l_z^{D_1}),l_z^i=W_z^i\bigodot z\\
  l_p=(l_p^1,l_p^2,...,l_z^{D_1}),l_p^i=W_p^i\bigodot p\\
  A\bigodot B = \sum_{i,j}A_{i,j}*B_{i,j}
  $$

- 交叉层的输入z是由Embedding层的向量生成的，$N表示x的域的个数，z代表FNN中的特征拼接，p代表特征组合，f_i \in R^M表示embedding向量$
  $$
  \begin{align}
  z&=(z_1,z_2,...,z_N)=(f_1,f_2,...,f_N)\\
  p&=(p_{i,j}),i,j \in 1,2,...,N\\
  p(i,j)&=g(f_i,f_j)
  \end{align}
  $$

- 文章在PNN的基础上定义了两种操作$g​$

  - 向量的点积即$<f_i,f_j>$，即$p$是一个$N*N$的矩阵。矩阵$p$的计算复杂度是$N*N*M$，则$l_p$的计算复杂度是$N*N*D_1$，总复杂度是

    $N*N*(D_1+M)$。由于W是对称的，受FM启发，将W分解，且维度与F相同。可以降低计算复杂度为$N*M*D_1$
    $$
    \begin{align}
    l_p^n &= W_p^n\bigodot p \\
    &= \sum_{i=1}^N\sum_{j=1}^N(W_p^n)_{i,j}p_{i,j}\\
    &=\sum_{i=1}^N\sum_{j=1}^N(W_p^n)_{i,j}<f_i,f_j>\\
    &= \sum_{i=1}^N\sum_{j=1}^N<\theta_i^n,\theta_j^n><f_i,f_j>\\
    &= <\sum_{i=1}^N\sigma_i^n,\sum_{i=1}^N\sigma_i^n>\\
    \end{align}
    $$

    其中：
    $$
    \sigma_i^n=<\theta_i^k,f_i>
    $$

  - 函数为$p(i,j)=f(f_i,f_j)=f_if_j^T$，总复杂度为$N*N*M*M*D_1$，论文重新定义了p矩阵，复杂度降低为了$(N+M)*M*D_1$
    $$
    p=\sum_{i=1}^N\sum_{j=1}^Nf_if_j^T=f_\sum(f_\sum)^T,f_\sum=\sum_{i=1}^Nf_i
    $$
    

#### 实验

- 对LR和FM对比模型采用L2
- 对NN采用Dropout
- 调节超参数 embedding层数，网络层数和激活函数