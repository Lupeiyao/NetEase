AFM是2017年提出的FM模型的Attention改进版。

#### 背景

FM解决了在稀疏数据中没有特征交叉的权重训练，权重不再是独立的。但这种方式对所有的交叉项取同样的权重，

实际情况中，一些无用交叉特征的权重应该是较低的，AFM通过引入Attention机制来解决这个问题。

#### 解决方案

![](C:\Users\Lunus\Desktop\推荐系统\图\AFM结构.jpg)

- 首先通过Embedding层，将输入的非零特征乘以$v$，其中$i \in \chi$，$\chi​$表示非零下标集合
  $$
  \epsilon = v_i * x_i
  $$

- 接着通过特征组合层，将非零特征两两交叉，$(i,j) \in R_{\chi}​$
  $$
  f_{PI}(\epsilon)= \{<v_i，v_j>x_ix_j\}\
  $$

- 接着通过attention base pooling + predicition score层预测，其中$P$为权重，$b$为偏置。
  $$
  \hat{y} = P^T\sum_{(i,j)\in R_\chi}a_{ij}<v_i,v_j>x_ix_j+b
  $$

- 如果直接利用上式进行训练的话，某些从未出现的特征组合的权重$a_{ij}$不会被更新，所以文章加入了一个单层MLP（Attention Net）来解决这个问题
  $$
  \begin{align}
  a_{ij}^{'}&=h^TReLu(W<v_i，v_j>x_ix_j+b)\\
  a_{ij}&=softmax(a_{ij}^{'})
  \end{align}
  $$
  其中$h \in R^t $，$W \in R^{t*k}$，$b \in R^t$是网络参数，$t$表示隐藏层的大小

- 最终的模型结构为
  $$
  \hat{y}_{AFM}(x)=w_0+\sum_{i=1}^{n}w_ix_i+P^T\sum_{i=1}^n\sum_{j=i+1}^na_{ij}<v_i,v_j>x_ix_j
  $$
  

#### 正则化

AFM比FM的表达能力强（参数多），比FM更容易过拟合，FM通过L2来正则，AFM则通过dropout+L2来正则。

注意：论文中dropout在特征组合层，在Attention Ne由于只是单层MLP，没有用dropout。