#### Discrete FM（DFM）

Discrete FM是山大、新加坡国立大学、中科大、南洋理工联合发表在IJCAI 2018上的文章，主要解决FM在一些特殊场景下模型容量和计算耗时偏大的问题。

在Yelp和Amazon上的实验证明DFM始终优于最先进的二值化推荐模型，相比于FM也显示出极具竞争力的性能。

#### 背景

当特征维度超过$10^7$时，FM会导致存储和计算成本非常大

#### 解决方案

FM中的隐向量$V\in R^{k \times n}$，每个元素时是实数值，而在DFM中，隐向量$B \in \pm 1^{k \times n}$。这样我们可以存储位矩阵、进行XOR位操作，而不用再进行浮点数的乘法，以达到快速推荐的目的。

$DFM(x):=w_0+\sum_{i=1}^{n}w_ix_i+\sum_{i=1}^{n}\sum_{j=i+1}^{n}\langle b_i,b_j\rangle x_ix_j$

优化目标：

$argmin_{w_0,w,B}\sum_{(\bf x,y) \in \cal V}(y-w_0-\sum_{i=1}^{n}w_ix_i-\sum_{i=1}^{n}\sum_{j=i+1}^{n}\langle b_i,b_j\rangle x_ix_j)^2+\alpha\sum_{i=1}^{n}w_i^2$

$s.t.  B \in \{\pm1\}^{k \times n},B1=0,BB^T=nI$

但是，这个想要优化这个目标是NP困难的，找到全局最优解需要$O(x^{kn})$的组合搜索。

于是，我们将优化目标进行平衡和去相关化的约束。为此，我们引进一个连续变量$D \in \cal D$，其中

$ \cal D$$ = \{ D \in \Bbb R^{k \times n}|D1=0,DD^T=nI\}$

优化目标改为：

$argmin_{w_0,w,B}\sum_{(\bf x,y)\in \cal V}(y-w_0-\sum_{i=1}^{n}w_ix_i-\sum_{i=1}^{n}\sum_{j=i+1}^{n}\langle b_i,b_j \rangle x_ix_j)^2+\alpha\sum_{i=1}^{n}w_i^2-2\beta tr(B^TD),$

$s.t. D1=0,DD^T=nI,B \in \{\pm1\}^{k \times n}$

然后基于此优化目标，提出了一种解决方案来解决混合整数优化问题，将原问题拆分成更新$B,D,\bf w$的三个子问题

##### $B$子问题

更新$B$中的每一个向量$b_r$，通过更新$b_r$向量中的每一位$b_{rt}$：

$b_{rt} \leftarrow sgn(K(\hat b_{rt},b_{rt}))$，

$\hat b_{rt}=\sum_{\cal V_r}(x_r\psi-x_r^2\bf \hat x^TZ_{\overline t}\bf b_{r \overline t}) \bf \hat x^T {\bf} z_t+\beta \mit d_{rt} $

#####$D$子问题

$D \leftarrow \sqrt n[\bf P,\widehat P][Q, \widehat Q]^T$

#####$w$子问题

$argmin_{w_0,\bf w} \sum_{(\bf x,y)\in \cal V}(\phi-w_0-\sum(w_ix_i))^2+\alpha\sum_{i=1}^{n}w_i^2$

$\phi=y-\sum_{i=1}^{n}\sum_{j=i+1}^{n}\langle b_i,b_j\rangle x_ix_j$

可以使用传统的FM算法对此子问题中的参数进行优化

#### 实验

较传统FM算法性能略有下降,但在Yelp和Amazon数据集上的速度均快了十倍以上。



