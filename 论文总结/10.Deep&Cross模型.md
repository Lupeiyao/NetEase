Deep&Cross是Google和Stanford在2017年提出的Deep Learning模型。

#### 背景

基于DNN的模型可以很好的捕捉高阶交叉特征，但如Wide&Deep等模型还需要做人工特征选择，DCN构造了一种免人工的模型。

#### Deep&Cross 解决方案

![](C:\Users\Lunus\Desktop\推荐系统\图\DCN架构图.jpg)

- 首先进行Embedding，连续特征进行归一化，离散特征进行Embedding，然后进行拼接，同时输入Corss和Deep两部分。
  $$
  \begin{align}
  x_0&=[x_{embed,1}^T,...,x_{embed,k}^T,x_{dense}^T]\\
  x_{embde,i}&=W_{embed,i}x_i
  \end{align}
  $$

- Deep Network是一个普通的MLP，每层之间全连接，不详细介绍了。
  $$
  h_{l+1}=f(W_lh_l+b_l)
  $$

- Cross Netword是关键创新，借鉴了残差网络的特性，x是每层的输出，w是每层的权重，b则是偏置，没层参数只有2*d个。
  $$
  x_{l+1}=x_0x_l^Tw_l+b_l+x_l=f(x_l,w_l,b_l)+x_l
  $$
  - Corss的特殊结构使得特征的交叉阶数随着Corss网络层数的增加而增加，$x_0$的最高交叉阶数在$l$层的网络中是$l+1​$阶。
  - 每层的神经元个数相同，都等于x_0的维度数，每层拟合的是x_l+1-x_l的残差。
  - Corss的设计完成了自动叉乘，且参数共享，最高层输出的每一维度都是很多高阶项的和（但并不是每一个高阶项都有一个权重），增加了模型的泛化能力。

- 最后Corss和Deep接一个sigmod。
  $$
  \hat{y}=\sigma([x_{L_1}^T,h_{L_2}^T]w)
  $$

- 损失函数
  $$
  loss=-\frac{1}{N}\sum_{i=1}^Ny_ilog(p_i)+(1-y_i)log(1-p_i)+\lambda\sum_l||w_l||_2^2
  $$

- 论文3.1对corss层的特征交叉特性作了介绍。这里就不赘述了。

#### 实验

- 在Criteo的数据集实验。有13个Integer特征，26个类别特征。共7天数据，11G。选6天训练，1天划分验证和测试。

- 对于类别特征，使用$6*(n)^{\frac{1}{4}}$选择Embedding维度，最终维度为1026.
- mini-batch为512，用Adam优化，使用Batch Normalization且gradient clip norm为100。
- L2和Dropout效果都不好，使用了early stopping防止过拟合。trainging step 是150000.
- 最优参数：Deep深度2-5，神经元个数32-1024，cross深度1-6，学习率10^-4 - 10^-3.
- 结果最优，且内存少于DNN40%。