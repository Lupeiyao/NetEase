# StandAlone 
利用spark自带的资源管理器进行集群管理，主节点为集群管理节点，Slave节点负责任务执行，Driver节点负责从管理节点申请资源并下发任务到Slave节点。

- 关闭防火墙 (每台)
```
service iptables stop   
chkconfig iptables off
```
- 修改/etc/hosts(每台)
```
ip1 master
ip2 slave1
ip3 slave2
```
- 配置SSH(免密码启动集群)
```
ssh ssh-keygen -t rsa
scp ~/.ssh/id_ras.pub root@slave1:~/.ssh
mv ~/.ssh/id_ras.pub ~/.ssh/authorized_keys
chmod 700 /root/.ssh     
chmod 600 /root/.ssh/authorized_keys
```
- 安装JAVA，SCALA ，配置JAVA_HOME，SCALA_HOME
- 安装SPARK
- 配置spark/conf/slaves(每台)
```
slave1
sLave2
```
- 配置spark-env.sh(每台)
```
export JAVA_HOME=/home/cloud/jdk1.8.0_101
export SCALA_HOME=/home/cloud/scala-2.10.6
export SPARK_LOCAL_HOSTNAME=`hostname`
```
- 启动 spark/sbin/start-all.sh
- 查看UI   Master_IP：8080 
- 测试Spark 
```
cd /spark/bin
./run-example SparkPi 10
```
- 关闭SPARK 
```
sbin/stop-all.sh
```
