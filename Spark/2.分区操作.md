## PairRDD的分区
- 举例
```

//join会导致shuffle操作（在join操作中需要进行笛卡儿积的遍历，既user*events的遍历
//spark会利用hash操作将具有相同hash的数据存放在同一台机器（降低遍历复杂度）
//但是还是会导致数据混洗（shuffle，数据跨节点交换，慢）

//userData存储用户信息，events存储用户阅读topic日志
userData = PairRDD[userId,userInfo]
events = PairRDD[userId,topic]

//计算用户浏览非兴趣topic个数
def process(userData : PairRDD, eventsData : PairRDD) = {
    val joined = userData.join(events)   
    joined.filter{ case(is,(info,topic) => !info.topics.contains(topic)}
        .count()
}
```
- 改进
```
//将userData利用Hash重新分区（而不是利用原始的分区）
//这样在join操作时，只会将events按照hash匹配到user的分区
//而不会重新shuffle UserData，如果userData较大且不怎么变化
//对于多次调用userData.join(events)会有明显的性能提升

userData.partitionBy(new HashPartitioner(100)).persist()
for(每五分钟一次)
    userData.join(events)
```

- Spark分区
```
//(join,cogroup,groupWith,groupByKey
//reduceByKey,combineByKey,lookup会利用分区，可以从中受益)

//spark保证相同键的数据出现在同一节点上
rdd.partitioner  -> Option[spark.Partitioner]
rdd.partitionBy(new org.apache.spark.HashPartitioner(num)).persist()
```

- PageRank分区改进
```
val links = RDD[id,list[id]]
val ranks = RDD[id,int]

links.partitionBy(new HashPartitioner(20)).persist()

for(i <- 0 until 10) {
    //hash分区可以优化join操作的时间，但不优化flatMap
    val contributions = links.join(ranks).flatMap{
        case (id,list,value) => list.map(linkId => (linkId,value / list.size)
    }
    //reduceByKey后的RDD是Hash分区的
    val ranks = contributions.reduceByKey((x,y)=>x+y)
        .mapValues(value => 0.15 + value * 0.85)
}
```

- 自定义分区
```
class MyPartitioner(numParts : Int) extends Partitioner {
    //分区数
    overridedef numPartitions : Int = numParts
    //分区方法，返回[0-numPartitions)之间的整数
    override def getPartition(key : Any) : Int = {
        val domain = key -> Int
        val code = (domain.hashCode % numPartitions)
        if(code < 0) code + numPartitions
        else code
    }
    //定义分区器的equals方法
    override def equals(other : Any) : Boolean = {
        other math {
            case dnp : MyPartitioner =>
                dnp.numPartitions == numPartitions
            case _ => false
        }
    }
}
```
