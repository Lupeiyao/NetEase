# spark程序

- 启动Spark
```
val conf = new SparkConf().setMater("local").setAppName("app_name")

val spark = SparkSession.builder.config(conf).getOrCreate()
val sc = spark.SparkContext

spark.stop()
```

- 创建RDD  
```
val dataRDD = sc.parallelize(List(1,2,3))
val dataRDD = sc.textFile("path")
val dataSet = spark.read.textFile("path")
dataSet.cache()
```

- spark算子

1.转化算子（懒加载）

```
data.map(x => x * x)
data.flatMap(x => x.split("_"))
data.filter(x => false == x)
data.sample(false,0.5)//不重复采样

//去重，并集，交集，集合减法，笛卡尔积
data.distinct()//混洗
data.union(other)//不混洗
data.intersection(other)//去重，混洗
data.subtract(other)//去重，混洗
data.cartesian(data)//不去重
```

2. 行动操作（会导致数据计算，每个行动操作都会重新计算），在行动操作之前cache，一定要提前思考分区合并的时候会发生什么，最好使用单位元素
```
data.collect()
data.count()
data.countByValue()
data.take(num)
data.top(num)//默认顺序
data.takeOrdered(num)(ordering)
data.takeSample(false,num)
data.foreach(fun)

//注意，reduce会在每个分区计算完后对每个分区结果再执行一次操作进行合并，一个分区的时候不合并
data.reduce((x,y) => x + y * y)

//fold同reduce，而且在一个分区的时候会使用10合并
data.fold(10)((x,y) => x + y * y)

//aggregate操作也会在合并时重复使用10
data.aggregate(10)(opsFun,combinFun)
```

3.RDD隐式转换

```
import org.apache.spark.SparkContext._
//将普通RDD转换为DoubleRDDFunctions,PairRDDFunctions
data.mean()
data.variance()
data.reduceByKey....
```

4. 持久化
```
data.cache()
//persist不会触发求值，memeory_only会在内存不够时按照LRU丢弃缓存，m_and_d会在内存不够时写入磁盘
data.persist(StorageLevel.MEMORY_ONLY|MEMORY_AND_DISK|DISK_ONLY)

```

- 传递函数

```
class Functions(val value : Int){
    def addOne(value : Int) : Int = { value + 1}
    
    def passFun(rdd : RDD[Int]) : RDD[Int] = {
        //相当于传递this.addOne
        rdd.map(addOne)
        //相当于传递this.value
        rdd.map(x => x + value)
        //将字段变量存储在临时变量
        val temp = value
        rdd.map(x => x + temp)
    }
}
```
