CTR估计的常用方法有，人工抽特征+LR，GBDT+LR，FM和FFM，FM+DNN。

FM是2010年提出的CTR模型，可以捕捉二阶的交叉特征，且在稀疏数据上表现良好。

#### 背景

FM（Factorization Machines）主要解决稀疏数据的特征组合问题，原因是业界特征非常多，离散变量在One-Hot编码后特征可能比样本都多。


普通的线性模型没有考察特征的相互关系，实际生活中，特征一定有相互关系，比如男性和女性对于化妆品的感兴趣程度显然不同。

#### 简单Ploy2

一种简单的方法是直接对特征作乘法。
$$
\check{y}(x) = w_0 + \sum_{i=1}^{n}w_ix_i + \sum_{i=1}^{n}\sum_{j=i+1}^{n}w_{ij}x_ix_j
$$
其中$n$是特征维度，$x_i$是特征，$w$是参数，$w_{ij}$只有在训练数据两个特征都不为零时才有梯度，这会导致欠拟合，从而模型训练不好。

#### FM解决方案

交叉项可以用矩阵分解来近似，二阶交叉项的权重矩阵$W\in R^{n*n}$是实对称矩阵。对任意正定矩阵$W$，只要$k$足够大，就存在：
$$
W=VV^T,V\in R^{n*k}
$$
俺么原方程可以转换为，其中$<>$是内积操作

$$
\check{y}(x) = w_0 + \sum_{i=1}^{n}w_ix_i + \sum_{i=1}^{n}\sum_{j=i+1}^{n}<v_i,v_j>x_ix_j
$$

对公式进行变换

$$
\begin{align}
\sum_{i=1}^{n}\sum_{j=i+1}^{n}<v_i,v_j>x_ix_j 

& =\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}<v_i,v_j>x_ix_j-\frac{1}{2}\sum_{i=1}^{n}<v_i,v_i>x_ix_i \\

& =\frac{1}{2}(\sum_{i=1}^n\sum_{j=1}^n\sum_{f=1}^{k}v_{if}v_{jf}x_ix_j-\sum_{i=1}^n\sum_{f=1}^{k}v_{if}v_{if}x_ix_i) \\

& =\frac{1}{2}\sum_{f=1}^{k}((\sum_{i=1}^nv_{if}x_i)(\sum_{j=1}^{n}v_{jf}x_j)-\sum_{i=1}^nv_{if}^2x_i^2) \\

& = \frac{1}{2}\sum_{f=1}^{k}((\sum_{i=1}^nv_{if}x_i)^2-\sum_{i=1}^nv_{if}^2x_i^2) \\
\end{align}
$$

此时计算复杂度由O(kn*n)降低为O(kn)

#### 预测

可以作回归（MSE），二分类(Logit loss)，或者Ranking(pairwise)

一般需要提供L2正则化防止过拟合

#### 最优化方法

一般利用梯度下降来进行，梯度


$$
f'(\theta)=
\begin{cases}
1& \theta=w_0\\
x_i& \theta=w_i\\
x_i\sum\limits_{j=1}^{n}v_{jf}x_j-v_{if}x_i^2& \theta=v_{if}
\end{cases}
$$

其中第三项求和项也和i无关，可以在前馈的过程中计算出来。这样每一项梯度都可以在常数时间内求解。训练复杂度也是O(kn)

#### 多阶扩展


2阶FM可以较容易的泛化到高阶：


$$
\check{y}(x) = w_0 + \sum_{i=1}^{n}w_ix_i + \sum_{l=2}^{d}\sum_{i_1=1}^{n}...\sum_{i_l=i_{l-1+1}}^{n}(\prod_{j=1}^{l}x_{i_j})(\sum_{f=1}^{k_l}\prod_{j=1}^{l}v_{i_j,f}^{(l)})
$$

其中d表示扩展到几阶，第L个参数是由PARAFAC模型的参数因子分解得到的


$$
V^l\in R^{n*k_l},k_l\in N_0^+
$$
#### FTRL

FTRL是Google在2013年放出这个优化方法，该方法有较好的稀疏性和收敛性。FTRL是一个在线学习的框架，论文中用于求解LR，具体求解方法如下图：

![image](https://pic1.zhimg.com/v2-a74f2deeea7eb48212a72d13315f4e0c_r.jpg)

只需要把论文中的伪代码进行修改，即可用于FM的参数求解。伪代码如下：


![image](https://pic2.zhimg.com/v2-f906a4d4a867eacf8dad6a8eb55d06bd_r.jpg)

#### 总结

FM模型的两个优势：

- 在高度稀疏的情况下特征之间的交叉仍然能够估计，而且可以泛化到未被观察的交叉，由于$w_{ij}$是由$v_i \cdot v_j$确定的，假设

  两个特征$i,j$没在训练集中出现，但与其他的特征同时出现，那么$v_i \cdot v_j$就不是0或者随机数

- 参数的学习和模型的预测时间复杂度是线性的

FM的优化点：

- 特征为全交叉，耗费资源，通常user与user，item与item内部的交叉的作用要小于user与item的交叉
- 使用矩阵计算，而不是for循环计算
- 高阶交叉特征的构造
