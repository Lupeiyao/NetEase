AutoInt是2018年提出的基于Multi-head self attention的DNN模型。

#### 背景





#### AutoInt解决方案

![](C:\Users\Lunus\Desktop\推荐系统\图\AutoInt架构图.jpg)

- 首先将离散变量One-Hot，连续变量归一化。接着每个域(离散和连续)进行Embedding，获得$e_i$表示域$i$的Embedding向量。

  - 离散变量的Embedding
    $$
    e_i = V_ix_i
    $$
    

  - 连续变量的Embedding
    $$
    e_m=v_mx_m
    $$

- 接着对每个Embedding向量进行重新表示到一个新的注意力子空间（d为Embedding维度，$d^{'}$为新空间的维度），用所有向量先映射再加权求和。权重由两两特征计算映射后的点积+softmax计算。

  ![](C:\Users\Lunus\Desktop\推荐系统\图\AutoInt计算图.jpg)
  $$
  \begin{align}
  \check{e}_m^{(h)} &= \sum_{i=1}^M\alpha_{m,k}^{(h)}(W_{Value}^{(h)}e_k),W\in R^{d^{'}*d}\\
  \alpha_{m,k}^{'(h)} &= \varphi ^{(h)}(e_m,e_k)=<W_{Query}^{(h)}e_m,W_{Key}^{(h)}e_k>\\
  \alpha_{m,k}^{(h)} &= softmax(\alpha_{m,k}^{'(h)})=\frac{\alpha_{m,k}^{'(h)}}{\sum_{l=1}^M(\alpha_{m,l}^{'(h)})}
  \end{align}
  $$
  

- 再然后将h个注意力子空间的结果进行拼接，并利用残差网络保留一些原始信息输入下一层，得到一个域在一层网络的输出。
  $$
  \check{e}_m = \check{e}_m^{(1)} \bigoplus \check{e}_m^{(2)} ... \check{e}_m^{(H)}\\
  {e}_m^{Res} = ReLU(\check{e}_m+W_{Res}e_m)
  $$
  

- 最后将每个域的输出拼接，加一个sigmod。这是一层的，可以通过叠加层来实现复杂映射。
  $$
  \hat{y}=\sigma(w^T(e_1^{Res} \bigoplus e_2^{Res} ... e_M^{Res}) + b)
  $$
  

#### 实验

- 删除出现次数少的特征
- 数值特征转换为log2 if value > 2